---
title: 动态延时任务总结
tags:
  - 任务调度
  - 延时任务
categories: [ 系统设计 ]
author: xyhao
keywords: 之所以要用延时任务就是为了将更新数据库的时间延时到下一个文件片更新之后，然后将延时的时间继续向后延，避免反复请求数据库。个人理解这是一种动态延时任务。
description: 之所以要用延时任务就是为了将更新数据库的时间延时到下一个文件片更新之后，然后将延时的时间继续向后延，避免反复请求数据库。个人理解这是一种动态延时任务。
top_img: >-
  https://gitee.com/xyhaooo/picrepo/raw/master/articleCover/2024-10-02-fileUpLoad.png
cover: >-
  https://gitee.com/xyhaooo/picrepo/raw/master/articleCover/2024-10-02-fileUpLoad.png
comments: true
abbrlink: d560d09d
date: 2024-10-02 00:00:00
---

> 之所以延时任务就是为了将更新数据库的时间延时到下一个文件片更新之后，然后延时任务刷新，避免反复请求数据库。
>

<h2 id="HNCfK">背景</h2>
文件上传是一个很古老的业务了，业界在设计的时候都会规划三个功能：秒传、分片上传、断点续传。

<h3 id="IOg4G">MD5密码杂凑算法</h3>
在介绍这三点之前，需要先了解MD5算法。这是一种密码杂凑算法，对于给定的任何输入，都会输出128位固定长度的16进制数字。既然长度固定，就会有重复的风险。国产算法SM3就是MD5的改良版，它优化了加密流程，也将输出的比特数增加到256位。

MD5重复的问题不是本文的重点，因此在接下来的叙述中不会考虑MD5重复的问题。

针对于MD5算法，我们任意输入文件名不同但文件内容相同的文件，都能得到相同的MD5值。这就是秒传的原理。如果数据库中已经存在客户端传过来的MD5值，那就说明以前有人已经上传过这份文件了，只需要新增一条用户和文件的对应关系即可。可以通俗的理解为：MD5值就是文件的身份证。

<h3 id="qG3Fb">表结构设计</h3>
很明显，用户和文件之间是多对多的关系。

我思考过两种表结构：

1. 只建立用户表和文件表，在文件表里关联用户表主键。
2. 建立用户表、文件表、用户文件关联表。

考虑到网盘的场景，文件内容相同的文件虽然文件名不同，但它们实际上还是同一份文件。

<h4 id="chUQJ">采用第一种方案</h4>
<h5 id="mGYkV">缺点：</h5>
MD5不能做主键，放着天然的主键属性不要，得让业务去生成主键。假如说有一个文件`程序员的自我修养.txt`，A用户上传完成后，在文件表增加一条
`程序员的自我修养.txt->A`的一行数据。B把它更名为`自我修养.txt`上传，虽然文件名不同，MD5值却是相同的。但是文件表里肯定要再插入一条数据
`自我修养.txt->B`的记录。MD5做主键就会冲突。但是这不是什么要紧的事。

<h5 id="W5ezw">优点：</h5>
相比于第二种方案，可以少建一张表，且避免了回表查询。

<h4 id="N6wWV">用第二种方案</h4>
<h5 id="Vaj5z">缺点：</h5>

1. **MD5还是不能做主键**。在这种方案里，如果文件表用MD5做主键，同一份文件只有一行数据，A对这个文件的文件名做了修改，在B用户看来，文件名是被同步修改的。B用户肯定会觉得碰上鬼打墙了，啥都没干文件名就被改了。
2. **要多建一张表，且会导致回表查询。**查询某个用户上传的全部文件。如果是建立关联表的话，得先查关联表，拿到文件ID列表再去做回表查询。但如果是在文件表加用户字段，只需要查询
   `where userID ＝ xxx`就可以把用户上传的全部文件查出来。

<h5 id="oUDll">优点：</h5>
相比于第一种方案并没有什么优点。

<h3 id="pruJk">三个功能</h3>

+ **秒传：**发现数据库里已经存在相同MD5值的文件时，去文件表新增一行数据，判定此次上传完毕。
+ **分片上传：**将文件划分为多个子部分，分别上传每个子部分，是断点续传的实现原理。
+ **断点续传：**如果上一次上传被中断需要重新上传，不需要从头开始上传，而是从上一次已上传的最新部分开始上传。

<h3 id="P5XN7">总结</h3>
虽然由于场景限制导致MD5这一天然的主键属性不能作为主键，这是产品特性决定的，同一份文件可能在不同用户的视角下名字不一样。但是MD5这个属性可以建立非唯一索引。

总体来看，流程比较简单，但是深究起来其中还是有不少可以挖掘的业务问题。

<h2 id="BPwjb">没上传完怎么办？</h2>
<h3 id="kO2gu">上传后的文件片怎么处理？</h3>
服务端需要将上传的那些文件片按照一定的规则（年-月）保存在临时目录中。出于需要实现断点续传的考虑，临时目录不能立马删掉。得保存一段时间。

+ 后续由延时任务将它删除吗？我认为并不需要，临时目录的删除并不需要很高的时间精准度，犯不着用延时任务增加系统复杂度。
+ 那把删除临时目录的任务发送到MQ？也没有必要。在MQ不出现消息积压的情况下，几秒钟内消息就会被消费。破坏了断点续传。

我认为最好的办法就是在月初统一删除上一个月的临时目录。比如第一级文件夹是xxxx年，第二级是xx月，里面保存了所有在xx月上传的文件片(
当然根据需要也可以精确到天，增加三级目录xx天)
。虽然会导致极端场景下断点续传失效（月底的晚上开始上传），但没必要为了极端场景做非常多优化。这些优化本身的成本可能比极端场景发生后的损失还大。技术总是服务于业务。（那我现在写的就是极端场景下恶意上传打垮服务器的解决方案，为什么要考虑它的解决方案？不考虑解决方案我怎么写简历？）

<h3 id="htm8V">有人恶意攻击怎么办</h3>
前面说到文件片按照一定的规则（年-月）保存在临时目录中，月初才清理。

且文件片只有在全部上传完毕后才会合并，并更新用户已使用空间。

**假如现在硬盘资源有限，有人从月初开始攻击服务器，具体表现为上传文件时不上传完。那么这些上传的文件碎片就会白白占据服务器硬盘资源而上传它们的用户的已使用空间却不会增加。久而久之用户上传的文件片已经远远超过他们的额度，服务器硬盘资源会被打垮
**。怎么解决？

<h3 id="H4MY4">解决方案</h3>
每个文件片上传时都去数据库里增加用户已使用空间吗？这会不会给数据库带来压力呢？数据库是整个服务端的最后方的组件，将数据持久化至硬盘，因此数据库本身的QPS完全比不上缓存。所以不能随随便便就请求数据库。

那就引入缓存？

将用户的已使用空间保存在redis里面？那是不是要考虑和数据库的一致性呢？redis虽然本身有备份机制，**
但他的备份机制并不能保证完全不丢数据。如果需要保证和数据库的一致性，**那和直接更新数据库没有任何区别，因为这些请求全部是写请求。

还有一个问题。用户上传到一半手动取消上传了，从用户角度考虑，用户肯定不希望在查看已使用空间时看到取消上传的文件还占据着已使用空间。这就带来一个新的问题：**
服务器视角下的已使用大小是已上传完毕的文件总大小加上未上传完毕的总大小。用户视角下的已使用大小是他已经上传完毕的那些文件的总大小（**
实际使用中发现百度网盘、夸克网盘等等都是这样做的）。这就需要我们准备两套计算已使用大小的方案。

在用户表里用两个字段分别表示已上传完毕文件总大小以及未上传完毕总大小。前者只在文件片合并和删除文件时更新即可，*
*后者则是每有一个文件片上传都需要更新**。

服务器在判断是否超出限额时需要将两者相加。

在 Redis 中记录用户的这两个字段。每个文件片更新时更新未上传完毕总大小。然后保证和数据库的一致性。

但是这样做和直接更新数据库还是没有区别。有多少文件片被上传就要更新多少次数据库。能不能减少更新次数？

注意到即使更新无数次数据库，每一次更新都可以覆盖前一次更新。

<h4 id="z58sA">举个例子</h4>
第一次更新是将未上传完大小`100M`加上这次文件片的大小`5M`得到`105M`；第二次更新是将未上传完大小`105M`加上这次文件片的大小
`5M`得到`110M`。更新了两次数据库。如果在第二次更新时加上`5M * 2 = 10M`，再把第一次更新删除，也就是`100M + 10M = 110M`
。这样做只更新了一次数据库。

**因此考虑引入延时任务,每个文件片上传后，在上一个延时任务执行之前刷新延时任务的执行时间，将它接着向后延。让最后一次更新覆盖之前所有更新，做到只更新一次数据库。
**

<h3 id="i6IQP">延时任务</h3>
<h4 id="d614D">存储延时任务的结构</h4>
<h5 id="STJl2">列表</h5>
插入：O(1)

查询：O(logN)

实现：我们可以利用列表去存即将触发的任务信息，通过遍历的方式去取到大于当前时间的任务，并且触发。

优点：实现简单

缺点：但需要对所有任务进行遍历，查出很多无效数据，极其低效。

<h5 id="82197238">大顶堆</h5>
删除：O(logN)

查询：O(1)

实现：我们也可以利用大顶堆的性质，每次都取堆顶元素，如果堆顶元素大于当前时间，那么就取最大元素。其余元素会利用大顶堆的性质，继续浮出最大的元素，然后继续比较。

优点：查询快，只会查到快到时间的任务，实现简单。

缺点：需要维护自身堆的性质，cpu压力高，无法抗住高并发。

<h5 id="19dd3d1e">B+树</h5>
查询：O(logN)

B+树（B-plus tree）是一种自平衡的树数据结构，它能够保持数据有序，允许插入、删除和查找操作在对数时间内完成。B+树特别适合于磁盘或其他直接存取辅助设备的存储系统，因为它能够最大化地减少I/O操作次数。

<h5 id="b9130034">跳表</h5>
查询：O(logN)

跳表（Skip List）是一种基于有序链表的高效数据结构，它通过在链表的基础上增加多级索引来实现快速的查找操作。跳表允许在对数时间内完成搜索、插入和删除操作，且插入和删除操作不需要频繁调整数据结构。

<h5 id="cc1d28d4">小总结</h5>
总的来说，列表和大顶堆由于自身的性质，并不适合这样的场景。对于扫表+触发的模式，其实本质是需要一个能高速范围查询的数据结构。

B+树和跳表都是高效的能范围查询数据结构，但它们各自适用于不同的场景。B+树更适合于磁盘存储和范围查询，而跳表则更适合于内存中的快速查找和分布式环境。

<h4 id="3c460321">存储数据库分析</h4>
我们举出基于内存的数据库的代表Redis和基于磁盘的数据库进行分析。

<h5 id="redis-vs-mysql">Redis VS MySQL</h5>
1.Redis的底层是跳表，而MySQL的底层是B+树。就范围查询而言，两者不分伯仲。

2.但Redis没有事务概念，内部实现是单线程，没有锁竞争，再加上IO多路复用的特性和极其高效的数据结构实现，就注定单机qps要远超过mysql。

3.mysql在这个场景下的优势则是有持久化能力，不容易丢数据，Redis可能在RDB和AOF的过程中有丢数据的可能性。

因此，mysql和redis都有可能是作为存储任务的数据库，需要区分场景。

综合考虑下我选用Redis。虽然Redis有丢数据的风险无法完美保证延时任务不丢，但是在本文中描述的延时任务是不断更新的，是动态的。比如某条数据现在是10M，然后执行RDB备份。再然后更新延时任务变成11M，假设此时断电宕机，11M这个数据没保存下来，但是10M这个数据已经保存了啊。虽然丢了一点数据，但是不多。系统停机维护时把延时任务和临时目录全删了，再把数据库中记录用户未上传完大小的这个字段置为0。一切又回到最初的起点。

> 问题又来了，既然这里可以接受丢数据，那为什么不把未上传完大小这个字段放在 redis ，然后不保证和 mysql 的一致性？或者说只有
> redis 里放未上传完大小，mysql 表字段里只有用户已上传完大小。都是相同的丢数据风险。为什么这里可以接受上面不可以接受？
>
> 其实这个需求就不是个合理的需求。只是我需要通过这个需求找到一个可以写简历的亮点。恰好对任务调度比较熟悉，所以需要通过这个不合理的需求引入任务调度。
>



如果用mysql存储动态延时任务，那就要经常更新mysql。其实也可以。但是mysql扛不住比较大的QPS，速度不如redis。综合考虑我还是用redis存延时任务。

<h4 id="Whhk8">缓存一致性问题</h4>
只要涉及到了缓存和数据库就一定会有缓存一致性问题，延时任务也不例外。

这里用的策略是先更新缓存，再更新数据库。

数据库的更新取决于延时任务被执行的时间。

通过延时任务的执行来保证 redis 和 mysql 的最终一致性。

<h4 id="DbWTf">延时任务的具体思路</h4>
文件片被上传时，用一个redis结构保存延时任务。`unfinishedFileSize`记录该文件未更新至数据库部分的大小。再用一个
`finishedFileSize`记录通过延时任务更新至数据库部分的大小。两个字段加起来表示该文件已上传完的大小。

```java
class Member {
    /*
        已重置为0的总大小，当一个延时任务被执行时，
        finishedDBSize += unfinishedDBSize;
        unfinishedDBSize = 0;
        
        该文件完全上传完毕后，将(finishedDBSize + unfinishedDBSize)从用户表中该用户
        的未上传完毕大小中减去
        问题：
        用户月初上传一半，月末上传另一半，是可以走断点续传的。
        月初上传的那部分已经通过延时任务保存至用户表的未上传完毕总大小50M。
        月末上传的那部分，又是一个新的延时任务，而这个新的延时任务只记录本次上传流程中
        的文件片大小50M。
        但是一个文件上传完了要从用户表的未上传完毕总大小字段中减去该文件的总大小100M。
        但是新延时任务只保存了月末上传的这部分文件片的大小50M。
        怎么办？
        断点续传时肯定得去保存文件片的临时目录里找到最大的文件片索引。那就顺便把该目录
        下的文件片总大小作为延时任务的finishedDBSize字段的初始值
        
        因为finishedDBSize代表已经更新至数据库的未上传完毕大小，unfinishedDBSize代表还没有更新到数据库的未上传完毕大小
        但是unfinishedDBSize一定会被延时任务给执行，所以先删和后删没区别
    */
    long unfinishedSize;// 已更新至数据库的未上传完成大小
    long unfinishedDBSize;// 待更新至数据库的未上传完成大小
    String userID;//当zset里的时间戳重复时，就要根据member里记录的用户id信息锁定唯一一条entry
    String md5;//标识唯一文件
    int flag;//标识当前延时任务是否被执行
}
```

定义zset中的member

![](https://gitee.com/xyhaooo/picrepo/raw/master/articleSource/2024-10-02-fileIUpLoad/img.png)

zset 中 member 的整体结构设计

```sql
create table user_info
(
    user_id              varchar(12) not null primary key,
    use_space_finished   bigint default 1024 comment '已上传完毕文件使用总空间,单位byte',
    use_space_unfinished bigint default 0 comment '未上传完毕文件使用总空间,单位byte',
    total_space          bigint comment '总空间'
)
```

用户表相关字段

某文件第一个文件分片上传完后，先设置`unfinishedFileSize`。根据当前时间戳 所属的分钟，将任务放入具体的 zset （如
23-54）。md5+userid作为 key， 该文件已上传的文件片大小总和等信息作为`member`加入到`zset`中。再用一个 map 记录md5+userid 和
zset 编号（23-54）的映射。

> 为什么 不用 文件id作为key呢？
>
>
主要是考虑到在文件被完全上传完之前，确定id没有意义。除非客户端在上传一个文件时携带id，且保证发生断点续传时，第二次上传携带的id和第一次相同。但是满足这个条件的id的生成规则肯定和雪花算法不同，因为雪花算法生成的id和时间有关，但当前场景下需要时间无关。注意。这里的意思并不是说一个文件被重复上传时客户端生成的id前后都一样，而是在一次上传流程中，如果因为应用重启等原因出现了中断，中断后的上传请求中的文件id和中断前保持相同。重复上传时的id则必须不同。
>
> 文件只在完全被上传后才会为它生成 id，否则用 md5+userid表示
>
> 而且这篇文章是针对于服务端的，我假设客户端上传时不携带id信息。另外，我认为**服务端一个重要的原则就是不要相信客户端传来的数据
**，能做检验的数据一定要做检验。客户端传来的数据都是可以被篡改的。https虽然安全，但如果出现了中间服务器，https报文就变成明文了。
>

后续文件片上传时，去 map 里根据md5+userid 拿到保存任务的 zset 编号，根据当前时间所属分钟 + 5s 是否等于该 zset 所属的分钟得到
boolean 类型变量 x。 去这个 zset 里根据md5+userid 找到这个文件的任务。如果 x == true, 修改该任务。否则将该任务删除，移至下一分钟的
zset。

当所有文件片正常上传完触发文件片合并，然后数据库更新`use_space_finished`字段。

之后根据`map`通过`用户id + 文件md5值`获取 zset 编号，获取该`zset`中对应任务的`unfinishedDBSize`和`unfinishedSize`。

执行`use_space_unfinished` = `use_space_unfinished` - `unfinishedDBSize` - `unfinishedSize`。**这是补偿用户的操作**。

因为延时任务肯定会被执行的(`use_space_unfinished` = `use_space_unfinished` + `unfinishedDBSize`)，先减`unfinishedDBSize`
和后减`unfinishedDBSize`没区别。至于`unfinishedSize`
？只要这个文件的延时任务没有被执行过，它就是0。即使执行过也没关系，见[3.4 考虑断点续传，用户只上传了一部分就宕机了，然后恢复机器继续上传。但是此时这个文件在zset里的任务已经更新到数据库了。后续的一部分文件上传是什么流程？](#EG571)

（**延时任务能不能成功执行至关重要！！！最终一致性全靠它保证！！！**）

<h4 id="av8rA">总结</h4>
一个文件片上传完后，有六个步骤：

1. 如果没有这个文件 md5 + userid 的临时目录，那么创建任务，放入当前实际的 Zset，在 map 里记录 Zset 和 MD5+
2. 后续的文件片上传根据`文件md5值+用户id`去`map`里找到保存上一个文件片延时任务的 zset，在根据 md5 + userid 去 Zset
   里找到延时任务
3. 修改任务，根据当前时间 + 5s 决定任务修改后是否移至另一 Zset 。+ 5s 是考虑到调度器的调度时机。
4. 写`map`，更新`文件md5值+用户id`对应的 Zset 编号。
5. 执行延时任务，`use_space_unfinished` = `use_space_unfinished` + `unfinishedDBSize`
6. 文件上传完成，`use_space_unfinished` = `use_space_unfinished` - `unfinishedDBSize` - `unfinishedSize`

**第五步和第六步的执行顺序没有要求！谁先执行都可以！**

****

<h3 id="Ixq6i">延时任务执行调度</h3>
当任务量很大时，任务的执行调度也需要花心思设计。具体参考[高性能调度系统设计总结](https://juzi-xyhao.github.io/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/2024/08/30/%E9%AB%98%E6%80%A7%E8%83%BD%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%80%BB%E7%BB%93.html)

[延时任务的调度与下发执行](https://www.yuque.com/u41117719/ofy6xw/hc44qyn5c6uaqez2)

<h2 id="RsYpu">一些A&&Q</h2>
<h3 id="rYkkf">延时任务重复问题</h3>
一个文件多个分片上传，会造成该文件有多个延时任务。也就说一个分片一个延时任务。如何解决重复的问题？

只有第一个分片会创建延时任务，后续文件片根据 `MD5 + userid` 找到延时任务删除它，修改后再新增它即可。

<h3 id="Xmomy">用户开多个客户端同时上传，怎么保证延时任务更新正确？</h3>
`zset`中的延时任务只和某一个文件有关。开多个客户端上传多个文件，就会出现多个延时任务，彼此互不干扰，并不会有什么问题。

<h4 id="rKatT">开多个客户端上传同一份文件呢？</h4>
那就让他传。本质上和开多个客户端上传多个文件没有区别。只是服务器上会出现两份相同的文件。

而且这种情况出现概率非常小。

<h3 id="Y0hw1">用户开多个客户端同时上传，怎么保证不会超过限额？</h3>
有`finishedFileSize`和`unfinishedFileSize`两个字符串类型分别记录每个用户已上传完毕大小和未上传完毕大小。

去`redis`里增加一个`String`字段，用于记录该用户正在上传的所有文件的总大小，叫它`upLoadingFileSize`。

每个文件上传时，前端把文件总大小也传过来。更新`upLoadingFileSize`。

通过`unfinishedFileSize`+ `finishedFileSize`+ `upLoadingFileSize` > `total_Space`的结果就可以知道这个文件能不能上传了。

显示给用户看的已使用空间就是`finishedFileSize`。只把`finishedFileSize`返回即可。 

<h3 id="EG571">
考虑断点续传，用户只上传了一部分就宕机了，然后恢复机器继续上传。但是此时这个文件在zset里的任务已经更新到数据库了。后续的一部分文件上传是什么流程？</h3>
**举个场景例子并给出解决方案：**

某用户在数据库中`use_space_unfinished`字段是0。`zset`里`unfinishedDBSize`字段是10M，`finishedDBSize`字段是`0M`
。然后用户宕机，延时任务没有被刷新而是被触发，导致用户在数据库中的`use_space_unfinished`字段数据被更新为`10M`。

此时zset里`unfinishedDBSize`字段是`0M`，`finishedDBSize`字段是`10M`。用户重启客户端发生断点续传，下一个大小为`1M`
的文件片被上传了。于是写`zset`将`unfinishedDBSize`字段更新为`1M`，没有对业务产生影响。

假如这个大小为`1M`的文件片是最后一个文件片，上传完服务器就触发了文件片合并流程，并使用 `lua 脚本 ` 从`zset`
中获取到延时任务并删除它([3.5 如果文件被完全上传后取出延时任务补偿用户时能把延时任务删除，能显著降低延时任务的数量吧？](#bBKci))
，将其中`finishedDBSize = 10M`的值从用户在数据库中的`use_space_unfinished`字段中减去，也就是`use_space_unfinished`  =
`use_space_unfinished` - `finishedDBSize` = 10M -10M = 0M。

`unfinishedDBSize`没有被更新进数据库，但延时任务被删了也执行不了。相当于手动执行了。

<h3 id="bBKci">如果文件被完全上传后取出延时任务补偿用户时能把延时任务删除，能显著降低延时任务的数量吧？</h3>
我有过这个优化的思路，在延时任务中加入一个变量表示延时任务的执行状态。以上文中提到的场景为例：假如取出延时任务是`命令A`
，执行延时任务是`命令B`。在`redis`服务器那边，A执行完紧接着就是B执行，通过 `命令 A` 拿到延时任务后发现它还没被执行，于是我把它删了（
`命令C`，在`命令B`之后执行），用户在数据库中未上传完毕大小字段变成了`10M - 10M = 0M`
，不要以为延时任务被删了不会执行，保证了最终一致性。实际上延时任务还是执行了，用户在数据库中未上传完毕大小字段变成了
`0M + 1M = 1M`。最终一致性被破坏。

之所以会发生这种情况就是因为`ZREMRANGEBYSOCRE`命令在删除某个entry时不会把entry返回，得先根据`ZRANGEBYSOCRE`命令获取到延时任务，再通过
`ZREMRANGEBYSOCRE`删除延时任务。也就是说：命令A和命令C的组合执行不是原子操作。

为了实现这一点，可以考虑使用`lua脚本`来保证原子性。

<h3 id="bLWYL">为什么不用md5+userid 作为文件表主键呢？</h3>
因为用户可以一份文件上传多次，这就会出现主键重复问题

<h3 id="UiUQV">能否基于Kafka实现延时队列进而实现本文中的延时任务？</h3>
本文中的延时任务和其它延时任务不太一样，本文中的延时任务需要更新被延时的时间，是动态延时。

如果一个延时任务被发送到Kafka，那我就不能修改它了，不符合本文延时任务的场景。

如何用Kafka实现延时队列？
参考  
[Kafka实现延时队列](https://zhuanlan.zhihu.com/p/365802989#:~:text=%E5%9F%BA%E4%BA%8Ekafka%E5%A6%82%E4%BD%95%E5%AE%9E)

<h3 id="LSMoF">为什么延时任务不设计成一个用户一个任务而是一个文件一个任务？</h3>
  一个用户一个任务是更优的方案，可以显著降低任务的数量。这是一个优化点。

在一个用户一个客户端的情况下，一个用户一个任务和一个文件一个任务是一样的。

一个用户开多个客户端上传，则可以显著降低任务的数量。

 

```java
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.ExecutionException;

@SpringBootTest
public class DelayQueueTest {

    private KafkaConsumer<String, String> consumer;
    private KafkaProducer<String, String> producer;
    private volatile Boolean exit = false;
    private final Object lock = new Object();
    private final String servers = "";

    @BeforeEach
    void initConsumer() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "d");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, "5000");
        consumer = new KafkaConsumer<>(props, new StringDeserializer(), new StringDeserializer());
    }

    @BeforeEach
    void initProducer() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, servers);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        producer = new KafkaProducer<>(props);
    }

    @Test
    void testDelayQueue() throws JsonProcessingException, InterruptedException {
        String topic = "delay-minutes-1";
        List<String> topics = Collections.singletonList(topic);
        consumer.subscribe(topics);

        Timer timer = new Timer();
        timer.schedule(new TimerTask() {
            @Override
            public void run() {
                synchronized (lock) {
                    consumer.resume(consumer.paused());
                    lock.notify();
                }
            }
        }, 0, 1000);

        do {

            synchronized (lock) {
                ConsumerRecords<String, String> consumerRecords = consumer.poll(Duration.ofMillis(200));

                if (consumerRecords.isEmpty()) {
                    lock.wait();
                    continue;
                }

                boolean timed = false;
                for (ConsumerRecord<String, String> consumerRecord : consumerRecords) {
                    long timestamp = consumerRecord.timestamp();
                    TopicPartition topicPartition = new TopicPartition(consumerRecord.topic(), consumerRecord.partition());
                    if (timestamp + 60 * 1000 < System.currentTimeMillis()) {

                        String value = consumerRecord.value();
                        ObjectMapper objectMapper = new ObjectMapper();
                        JsonNode jsonNode = objectMapper.readTree(value);
                        JsonNode jsonNodeTopic = jsonNode.get("topic");

                        String appTopic = null, appKey = null, appValue = null;

                        if (jsonNodeTopic != null) {
                            appTopic = jsonNodeTopic.asText();
                        }
                        if (appTopic == null) {
                            continue;
                        }
                        JsonNode jsonNodeKey = jsonNode.get("key");
                        if (jsonNodeKey != null) {
                            appKey = jsonNode.asText();
                        }

                        JsonNode jsonNodeValue = jsonNode.get("value");
                        if (jsonNodeValue != null) {
                            appValue = jsonNodeValue.asText();
                        }
                        // send to application topic
                        ProducerRecord<String, String> producerRecord = new ProducerRecord<>(appTopic, appKey, appValue);
                        try {
                            producer.send(producerRecord).get();
                            // success. commit message
                            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(consumerRecord.offset() + 1);
                            HashMap<TopicPartition, OffsetAndMetadata> metadataHashMap = new HashMap<>();
                            metadataHashMap.put(topicPartition, offsetAndMetadata);
                            consumer.commitSync(metadataHashMap);
                        } catch (ExecutionException e) {
                            consumer.pause(Collections.singletonList(topicPartition));
                            consumer.seek(topicPartition, consumerRecord.offset());
                            timed = true;
                            break;
                        }
                    } else {
                        consumer.pause(Collections.singletonList(topicPartition));
                        consumer.seek(topicPartition, consumerRecord.offset());
                        timed = true;
                        break;
                    }
                }

                if (timed) {
                    lock.wait();
                }
            }
        } while (!exit);

    }
}
```